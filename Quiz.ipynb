{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Load the dataset\n",
        "data = pd.read_csv('AirPassengers.csv')\n",
        "\n",
        "# Display the first few rows of the dataset\n",
        "print(data.head())\n",
        "\n",
        "# Preprocessing: Extract the 'Passengers' column for normalization\n",
        "passengers = data['#Passengers'].values.reshape(-1, 1)\n",
        "\n",
        "# Normalization: Scale the passenger values between 0 and 1\n",
        "scaler = MinMaxScaler()\n",
        "passengers_normalized = scaler.fit_transform(passengers)\n",
        "\n",
        "# Replace the original 'Passengers' column with the normalized values\n",
        "data['#Passengers'] = passengers_normalized\n",
        "\n",
        "# Splitting the dataset into training and test sets (80% train, 20% test)\n",
        "train_data, test_data = train_test_split(data, test_size=0.2, shuffle=False)\n",
        "\n",
        "# Display the dimensions of the training and test sets\n",
        "print(\"Training set size:\", len(train_data))\n",
        "print(\"Test set size:\", len(test_data))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wkp44PUF7rwD",
        "outputId": "62aa2aa9-2776-4b5a-95b2-2bdde886cc2d"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "     Month  #Passengers\n",
            "0  1949-01          112\n",
            "1  1949-02          118\n",
            "2  1949-03          132\n",
            "3  1949-04          129\n",
            "4  1949-05          121\n",
            "Training set size: 115\n",
            "Test set size: 29\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from keras.models import Sequential\n",
        "from keras.layers import LSTM, Dropout, Dense\n",
        "\n",
        "def create_lstm_model(input_shape, lstm_units=[50, 50], dropout_rate=0.2):\n",
        "\n",
        "    model = Sequential()\n",
        "\n",
        "    # Adding the first LSTM layer\n",
        "    model.add(LSTM(units=lstm_units[0], return_sequences=True, input_shape=input_shape))\n",
        "    model.add(Dropout(dropout_rate))\n",
        "\n",
        "    # Adding additional LSTM layers if specified\n",
        "    for units in lstm_units[1:]:\n",
        "        model.add(LSTM(units=units, return_sequences=True))\n",
        "        model.add(Dropout(dropout_rate))\n",
        "\n",
        "    # Adding a final LSTM layer without return_sequences=True\n",
        "    model.add(LSTM(units=lstm_units[-1]))\n",
        "    model.add(Dropout(dropout_rate))\n",
        "\n",
        "    # Adding a Dense output layer\n",
        "    model.add(Dense(units=1))\n",
        "\n",
        "    # Compiling the model\n",
        "    model.compile(optimizer='adam', loss='mean_squared_error')\n",
        "\n",
        "    return model\n",
        "\n",
        "\n",
        "input_shape = (sequence_length, num_features)\n",
        "model = create_lstm_model(input_shape, lstm_units=[50, 50], dropout_rate=0.2)\n",
        "model.summary()\n"
      ],
      "metadata": {
        "id": "To9zW7dM8a-2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from keras.models import Sequential\n",
        "from keras.layers import LSTM, Dropout, Dense\n",
        "from keras.optimizers import Adam\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "def compile_and_train_model(model, train_data, epochs=100, batch_size=32):\n",
        "\n",
        "    X_train, y_train = train_data\n",
        "\n",
        "    # Compile the model\n",
        "    model.compile(optimizer=Adam(), loss='mean_squared_error')\n",
        "\n",
        "    # Train the model\n",
        "    history = model.fit(X_train, y_train, epochs=epochs, batch_size=batch_size, validation_split=0.2, verbose=1)\n",
        "\n",
        "    return history\n",
        "\n",
        "# Example usage:\n",
        "# Assuming input sequences of shape (sequence_length, num_features)\n",
        "input_shape = (sequence_length, num_features)\n",
        "model = create_lstm_model(input_shape, lstm_units=[50, 50], dropout_rate=0.2)\n",
        "\n",
        "# Assuming you have training data X_train, y_train\n",
        "train_data = (X_train, y_train)\n",
        "\n",
        "# Compile and train the model\n",
        "history = compile_and_train_model(model, train_data, epochs=100, batch_size=32)\n",
        "\n",
        "# Plot training and validation loss\n",
        "plt.plot(history.history['loss'], label='Training Loss')\n",
        "plt.plot(history.history['val_loss'], label='Validation Loss')\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Loss')\n",
        "plt.legend()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "dPD2ikMc9Dlm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import mean_absolute_error, mean_squared_error\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Assuming you have test data X_test, y_test\n",
        "test_data = (X_test, y_test)\n",
        "\n",
        "# Evaluate the trained model on the test set\n",
        "predictions = model.predict(X_test)\n",
        "mae = mean_absolute_error(y_test, predictions)\n",
        "rmse = mean_squared_error(y_test, predictions, squared=False)\n",
        "\n",
        "print(\"Mean Absolute Error (MAE):\", mae)\n",
        "print(\"Root Mean Squared Error (RMSE):\", rmse)\n",
        "\n",
        "# Visualize the model's predictions against the ground truth\n",
        "plt.figure(figsize=(12, 6))\n",
        "plt.plot(y_test, label='Actual')\n",
        "plt.plot(predictions, label='Predicted')\n",
        "plt.title('AirPassengers Forecasting')\n",
        "plt.xlabel('Time')\n",
        "plt.ylabel('Number of Passengers')\n",
        "plt.legend()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "r3mO3SpH_Eoo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from keras.wrappers.scikit_learn import KerasRegressor\n",
        "from sklearn.model_selection import RandomizedSearchCV\n",
        "import numpy as np\n",
        "\n",
        "# Define function to create LSTM model\n",
        "def create_lstm_model(learning_rate=0.01, lstm_units=50, dropout_rate=0.2):\n",
        "    model = Sequential()\n",
        "    model.add(LSTM(units=lstm_units, input_shape=(sequence_length, num_features)))\n",
        "    model.add(Dropout(dropout_rate))\n",
        "    model.add(Dense(units=1))\n",
        "    model.compile(optimizer=Adam(learning_rate), loss='mean_squared_error')\n",
        "    return model\n",
        "\n",
        "# Create KerasRegressor wrapper\n",
        "model = KerasRegressor(build_fn=create_lstm_model)\n",
        "\n",
        "# Define hyperparameter search space\n",
        "param_dist = {\n",
        "    'learning_rate': [0.001, 0.01, 0.1],\n",
        "    'lstm_units': [50, 100, 150],\n",
        "    'dropout_rate': [0.1, 0.2, 0.3]\n",
        "}\n",
        "\n",
        "# Perform random search\n",
        "random_search = RandomizedSearchCV(model, param_dist, n_iter=10, cv=3, scoring='neg_mean_squared_error', verbose=1)\n",
        "random_search_results = random_search.fit(X_train, y_train)\n",
        "\n",
        "# Print best parameters and corresponding mean test score\n",
        "print(\"Best Parameters:\", random_search_results.best_params_)\n",
        "print(\"Best Mean Test Score:\", -random_search_results.best_score_)\n"
      ],
      "metadata": {
        "id": "GvYW_cL9_h0b"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Challenges encountered during model training and optimization:\n",
        "\n",
        "One challenge is selecting appropriate hyperparameters, such as learning rate, batch size, number of LSTM layers and units, and dropout rate. Tuning these hyperparameters can be time-consuming and computationally expensive, especially when performing grid search or random search.\n",
        "Another challenge is dealing with overfitting, especially when working with limited data. Dropout layers and early stopping techniques are commonly used to mitigate overfitting.\n",
        "Decision on the number of LSTM layers and units:\n",
        "\n",
        "The number of LSTM layers and units is often determined through experimentation and validation. Starting with a single LSTM layer and gradually increasing the complexity by adding more layers and units can help find the optimal architecture.\n",
        "Factors such as the complexity of the dataset, available computational resources, and the trade-off between model complexity and generalization ability influence the decision.\n",
        "Preprocessing steps performed on the time series data before training the model:\n",
        "\n",
        "Common preprocessing steps include:\n",
        "Removing any trend and seasonality from the data through techniques like differencing or decomposition.\n",
        "Normalizing the data to a common scale to ensure stable training.\n",
        "Splitting the data into training and test sets, and possibly further dividing the training set into validation sets for hyperparameter tuning.\n",
        "Preparing the data into sequences suitable for input to the LSTM model.\n",
        "Purpose of dropout layers in LSTM networks and how they prevent overfitting:\n",
        "\n",
        "Dropout layers randomly deactivate a fraction of neurons during training, preventing them from contributing to the forward pass and backward pass. This regularization technique helps prevent overfitting by reducing the model's reliance on specific neurons and encourages the network to learn more robust features.\n",
        "Dropout layers provide a form of ensemble learning within a single model, effectively reducing the model's variance and improving generalization performance.\n",
        "Analysis of the model's ability to capture long-term dependencies and make accurate predictions:\n",
        "\n",
        "The LSTM architecture is specifically designed to capture long-term dependencies in sequential data. By maintaining a memory cell and gating mechanisms, LSTMs can effectively learn and remember patterns over long sequences.\n",
        "The accuracy of predictions depends on various factors such as the quality and quantity of training data, the complexity of the underlying patterns in the data, and the effectiveness of the chosen architecture and hyperparameters.\n",
        "Evaluation metrics such as mean absolute error (MAE) or root mean squared error (RMSE) can provide quantitative insights into the model's predictive performance.\n",
        "Potential improvements or alternative approaches for enhancing forecasting performance:\n",
        "\n",
        "Experimenting with more complex architectures, such as bidirectional LSTMs or stacked LSTMs, may capture more intricate patterns in the data.\n",
        "Feature engineering and incorporating domain knowledge can improve model performance by providing additional relevant information to the model.\n",
        "Ensembling multiple models or using techniques like model averaging can further enhance forecasting accuracy and robustness.\n",
        "Regularization techniques such as L1 and L2 regularization, in addition to dropout, can help prevent overfitting.\n",
        "Continual monitoring and fine-tuning of hyperparameters and model architecture based on validation performance can lead to better forecasting results over time."
      ],
      "metadata": {
        "id": "W7I7yuvlAV34"
      }
    }
  ]
}